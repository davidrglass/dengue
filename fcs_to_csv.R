##############################################################################
#
# Script: fcs_to_csv.R
# Project: Dengue
# Author: David Glass
# Date: 9-1-20
#
# This program takes in batch-normalized, singlet-gated fcs files from dengue
# patient PBMCs, asinh transforms, scales data, and writes out a csv
#
# Instructions:
# Update USER INPUTS with directory locations
# Run script
#
# Versions:
# R 3.6.3
# Rstudio 1.3.1093
# flowCore 1.52.1
# tidyverse 1.3.1
# data.table 1.14.0
# 
##############################################################################



###### LIBRARIES ######

require(flowCore)
require(tidyverse)
require(data.table)



##### INPUTS #####

path <- "~/Research/dengue/"
dump <- c("Time", "Event_length", "Viability", "DNA_1", "DNA_2", "DNA_3", "barium",
          "Barcode_A", "Barcode_B","Barcode_C", "Barcode_D", "Barcode_E", "Barcode_F",
          "I_127", "Os_184", "Os_187", "Os_188", "Os_189", "Pt_190", "Pt_192", "Au_197")
# finalized_fcs.csv placed by user
fcs.desc <- paste0(path, "tables/finalized_fcs.csv")
# taken from flowRepository; originally generated by batch_normalize.R
fcs.path <- paste0(path, "fcs/post_batch_correction/")
factors <- c("sample", "plate")



##### FUNCTIONS #####

readFiles <- function(p=fcs.path) {
  # takes in a path with fcs files and returns a list of data.tables of the expression matrix
  # Inputs:
  #   p - directory storing fcs files
  # Outputs:
  #   frames - a list of data.tables
  print("Reading files")
  files <- list.files(path=p, pattern=".fcs", full.names=FALSE, recursive=FALSE) %>%
    .[grep("PA1", ., invert=TRUE)] # drop normalization controls
  frames <- setNames(vector("list", length(files)), files)
  for (i in seq(files)) {
    fcs <- read.FCS(paste0(p, files[i]), transformation=FALSE, emptyValue=FALSE)
    frames[[i]] <- data.table(exprs(fcs)) %>%
      setnames(pData(parameters(fcs))$desc)
  }
  return(frames)
}


combineFiles <- function(frames, fd=fcs.desc, du=dump, fa=factors) {
  # Combines a list of data.tables into a single data.table with factor columns added
  # Inputs:
  #   frames - a list of data.tables
  #   fd - csv file location with fcs data
  #   dump - vector of dump channel names
  #   fa - vector of factors
  # Outputs:
  #   frame - a single data.table
  print("Combining files")
  d <- fread(fd, stringsAsFactors=T)
  all.cols <- unique(unlist(lapply(frames, colnames))) %>% .[!. %in% du] %>% c(fa)
  frame <- data.table(matrix(nrow=0, ncol=length(all.cols))) %>% setnames(all.cols)
  for (f in d$filename) {
    frame <- frames[[f]] %>%
      cbind(d[filename==f]) %>%
      .[, all.cols, with=F] %>%
      rbind(frame)
  }
  return(frame)
}


asinTransform <- function(dt, fa=factors) {
  # asinh transforms
  # Inputs:
  #   dt - data.table
  #   fa - character vector of channels not to transform
  # Outputs:
  #   dat - data.table
  print("Asinh transforming")
  to.transform <- colnames(dt)[!colnames(dt) %in% fa]
  dt[, (to.transform) := asinh(dt[, to.transform, with=F]/5)]
  return(dt)
}


scaleData <- function(dt=dat, fa=factors, quant=0.999) {
  # scales each run individually
  # Inputs:
  #   dt - data.table
  #   fa - character vector of channels not to scale
  #   quant - percentile of expression to scale to
  # Outputs:
  #   dt - data.table
  print("Scaling data")
  channels <- colnames(dt)[!colnames(dt) %in% fa]
  refs <- mapply(quantile, x=dt[, channels, with=F], MARGIN=2, probs=c(quant), na.rm=T)
  for (i in seq(channels)) {
    dt[, channels[i]:=dt[, channels[i], with=F] / refs[i]]
  }
  return(dt)
}



###### MAIN ######

dat <- readFiles() %>%
  combineFiles() %>%
  asinTransform() %>%
  scaleData()

# strip irrelevant characters from sample names
temp <- levels(dat$sample)
for (i in seq(temp)) {
  temp[i] <- substr(temp[i], start=10, stop=str_locate(temp[i], ".fcs")[, 1] - 1)
}
setattr(dat$sample, "levels", temp)
rm(temp)

fwrite(dat, paste0(path, "tables/processed_dat.csv"))
